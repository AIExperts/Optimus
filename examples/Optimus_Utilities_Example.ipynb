{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimus Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook decribes with more detail what Optimus Utitilies module is about. \n",
    "\n",
    "Utilities module contains tool classes that support use of DataFrameTransformer and DataFrameAnalyzer modules. It provides some facilities to read local/remote csv files (by providing an url), to load spark dataframes from an url and also, to read parquet files and provide csv-parquet conversion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>Starting or getting SparkSession and SparkContext.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>Setting checkpoint folder (local). If you are in a cluster change it with set_check_point_folder(path,'hadoop').</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting previous folder if exists...\n",
      "Creation of checkpoint directory...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ironmussa/Optimus\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ironmussa/Optimus/raw/master/images/robotOptimus.png\" style=\"float:left;margin-right:10px;vertical-align:top;text-align:center\" height=\"50\" width=\"50\"/>\n",
       "            </a>\n",
       "            <span><b><h2>Optimus successfully imported. Have fun :).</h2></b></span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import optimus\n",
    "import optimus as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation of Utility class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of Utilities class\n",
    "tools = op.Utilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataframe from csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets assume you have a file, called foo.csv, in your current directory (this file foo.csv actually can be found in the Optimus repository).\n",
    "\n",
    "The dataframe can be obtained by reading the csv file, just with the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading dataframe in this case, local file \n",
    "# system (hard drive of the pc) is used.\n",
    "\n",
    "df = tools.read_dataset_csv(path=\"foo.csv\", delimiter_mark=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can view part of the dataframe by using the Dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+---------+-------+-----+----------+--------+\n",
      "| id|firstName|   lastName|billingId|product|price|     birth|dummyCol|\n",
      "+---+---------+-----------+---------+-------+-----+----------+--------+\n",
      "|  1|     Luis|Alvarez$$%!|      123|   Cake|   10|1980/07/07|   never|\n",
      "|  2|    André|     Ampère|      423|   piza|    8|1950/07/08|   gonna|\n",
      "|  3|    NiELS| Böhr//((%%|      551|  pizza|    8|1990/07/09|    give|\n",
      "|  4|     PAUL|     dirac$|      521|  pizza|    8|1954/07/10|     you|\n",
      "|  5|   Albert|   Einstein|      634|  pizza|    8|1990/07/11|      up|\n",
      "+---+---------+-----------+---------+-------+-----+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or with the DataframeProfler Optimus class:\n",
    "\n",
    "In the following cell, a basic profile of the DataFrame is shown. This overview presents basic information about the DataFrame, like number of variable it has, how many are missing values and in which column, the types of each varaible, also some statistical information that describes the variable plus a frecuency plot. table that specifies the existing datatypes in each column dataFrame and other features. Also, for this particular case, the table of dataType is shown in order to visualize a sample of column content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instance of profiler class\n",
    "profiler = op.DataFrameProfiler(df)\n",
    "profiler.profiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools.read_dataset_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation of DataFrameTransformer\n",
    "DataFrameTransformer is a specialized class to make dataFrame transformations. Transformations are optimized as much as possible to internally used native spark \n",
    "transformation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of transformer class \n",
    "transformer = op.DataFrameTransformer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+---------+-------+-----+----------+--------+\n",
      "| id|firstName|   lastName|billingId|product|price|     birth|dummyCol|\n",
      "+---+---------+-----------+---------+-------+-----+----------+--------+\n",
      "|  1|     Luis|Alvarez$$%!|      123|   Cake|   10|1980/07/07|   never|\n",
      "|  2|    André|     Ampère|      423|   piza|    8|1950/07/08|   gonna|\n",
      "|  3|    NiELS| Böhr//((%%|      551|  pizza|    8|1990/07/09|    give|\n",
      "|  4|     PAUL|     dirac$|      521|  pizza|    8|1954/07/10|     you|\n",
      "|  5|   Albert|   Einstein|      634|  pizza|    8|1990/07/11|      up|\n",
      "+---+---------+-----------+---------+-------+-----+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints\n",
    "At this point, it is important to remember that Optimus is an apache spark upper abstraction layer. In consequence, all\n",
    "the computing logic is actually made by apache spark and this is a main reason why we need to use checkpoints. \n",
    "\n",
    "Checkpoints in this context are images of processed data (dataframes) saved in the disk. Checkpoints are useful because of the apache Spark lazy evaluation (instructions are not runned until they are actually needed), so when you execute for example, some dataframe transformations, what Apache Spark actually does is to place those instrucctions in a queue. \n",
    "\n",
    "The transformation instructions are acummulated but not executed until an action instruction is demanded (for example a df.show() instruction). So the problem cames when we have a very long list of stacked instrucctions. At this point executing all instructions everytime the dataframe is shown, can result tedius and time consumming. \n",
    "\n",
    "The solution is just to the DataFrameTransformer.check_point() method in order to cut the lineage of instrucctions and to save the new stated dataframe and to advoid re running processes again an again. All this issue is actually normal in Apache Spark because it has been created to not save the result, but the tasks (because it is also a distribute computing framework)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting checkpoint folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting previous folder if exists...\n",
      "Creation of checkpoint directory...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "tools.set_check_point_folder(path=\"/home/hugo/Documents/Development/Optimus/examples\", file_system=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting checkpoint folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting checkpoint folder...\n",
      "Folder deleted. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tools.delete_check_point_folder(path=\"/home/hugo/Documents/Development/Optimus/examples\", file_system=\"local\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
